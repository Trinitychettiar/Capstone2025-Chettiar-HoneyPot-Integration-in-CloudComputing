# -*- coding: utf-8 -*-
"""honeypot_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rcBpdEh9LcI4tDCs0bg1hlWdnPa1UcsB
"""

# 1) Install deps
!pip install torch pandas numpy scikit-learn matplotlib joblib

# 2) Run on your CSV (edit the path)
!python deep_attack_predictor.py --csv "C:/path/to/your/multicloud_honeypot_4weeks.csv"

!python.exe -m pip install --upgrade pip

import pandas as pd

# Load your CSV file
df = pd.read_csv("multicloud_honeypot_4weeks.csv")  # use full path if needed

# Confirm it's loaded
df.head()

import os, json, argparse, math, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay




import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import joblib

#  CLI
def get_args():
    p = argparse.ArgumentParser(description="Honeypot DL: Supervised + Unsupervised")
    p.add_argument("--csv", required=True, help="Path to honeypot CSV")
    p.add_argument("--label", default="event_type", help="Label column for supervised classification")
    p.add_argument("--time", default="timestamp", help="Timestamp column (adds hour/dow)")
    p.add_argument("--outdir", default="outputs", help="Directory to save artifacts/results")
    p.add_argument("--epochs", type=int, default=20, help="Classifier epochs")
    p.add_argument("--ae_epochs", type=int, default=15, help="Autoencoder epochs")
    p.add_argument("--batch", type=int, default=64, help="Classifier batch size")
    p.add_argument("--ae_batch", type=int, default=128, help="Autoencoder batch size")
    p.add_argument("--lr", type=float, default=1e-3, help="Classifier learning rate")
    p.add_argument("--ae_lr", type=float, default=1e-3, help="Autoencoder learning rate")
    p.add_argument("--val_frac", type=float, default=0.2, help="Validation fraction")
    p.add_argument("--seed", type=int, default=42, help="Random seed")
    return p.parse_args()

# Column auto-detection
CANDIDATE_COLUMNS = {
    "timestamp": ["timestamp","@timestamp","time","event_time"],
    "provider":  ["provider","cloud_provider"],
    "region":    ["region"],
    "honeypot":  ["honeypot","sensor","hp_name"],
    "src_ip":    ["src_ip","source_ip","src.ip"],
    "src_country":["src_country","source_country","geo_country","geoip.country_code2"],
    "asn":       ["asn","src_asn","autonomous_system_number"],
    "ip_reputation":["ip_reputation","src_ip_rep","reputation"],
    "src_os":    ["src_os","os","p0f_os"],
    "protocol":  ["protocol","proto"],
    "dst_port":  ["dst_port","destination_port","dst.p"],
    "bytes_in":  ["bytes_in","in_bytes","rx_bytes"],
    "bytes_out": ["bytes_out","out_bytes","tx_bytes"],
    "cve":       ["cve","vuln","suricata.cve"],
    "event_type":["event_type","label","attack_type"]
}

def first_present(df, cands):
    for c in cands:
        if c in df.columns: return c
    return None
def pick_cols(df: pd.DataFrame, mapping: Dict[str, List[str]]) -> Dict[str,str]:
    sel = {k:first_present(df, v) for k,v in mapping.items()}
    return {k:v for k,v in sel.items() if v is not None}

#AI Assistance: This model architecture and parts of the logic were developed with help from OpenAI's ChatGPT,
#               including dynamic embedding handling, adaptive hidden layer sizing.

#  Tabular DL model
class TabularNet(nn.Module):
    def __init__(self, cat_cols, cat_vocabs, num_dim, num_classes, emb_rule=lambda n: min(50,(n+1)//2)):
        super().__init__()
        self.cat_cols = cat_cols
        self.embs = nn.ModuleDict()
        emb_total = 0
        for c in cat_cols:
            vocab_size = len(cat_vocabs[c])
            e = emb_rule(vocab_size)
            self.embs[c] = nn.Embedding(vocab_size, e, padding_idx=0)
            emb_total += e
        in_dim = emb_total + num_dim
        hidden = max(64, min(256, in_dim * 2))  # adapt a bit to input size
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden), nn.ReLU(), nn.BatchNorm1d(hidden), nn.Dropout(0.2),
            nn.Linear(hidden, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
    def forward(self, x_cat, x_num):
        outs = [ self.embs[c](x_cat[:, j]) for j,c in enumerate(self.cat_cols) ] if len(self.cat_cols)>0 else []
        z = torch.cat(outs, dim=1) if outs else torch.zeros((x_cat.size(0),0), device=x_cat.device)
        if x_num.nelement() > 0:
            z = torch.cat([z, x_num], dim=1)
        return self.mlp(z)

#  Main
def main():
    a = get_args()
    os.makedirs(a.outdir, exist_ok=True)
    rng = np.random.RandomState(a.seed)
    torch.manual_seed(a.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    #  Load data
    df = pd.read_csv(a.csv)
    print(f"Loaded {len(df):,} rows from {a.csv}")

    SELECTED = pick_cols(df, CANDIDATE_COLUMNS)
    print("Detected columns:", SELECTED)

SELECTED = {k: k for k in df.columns}  # if you're not auto-selecting column names

# Normalize strings
for key in ["provider", "region", "honeypot", "src_country", "ip_reputation", "src_os", "protocol", "cve", "event_type"]:
    col = SELECTED.get(key, key)
    if col in df.columns:
        df[col] = df[col].astype(str).str.strip().str.lower()

# Numerics
    for key in ["dst_port","bytes_in","bytes_out","asn","hour","dow"]:
        col = SELECTED.get(key, key)
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
            df[col] = df[col].fillna(df[col].median())

# Parse timestamp and extract time features
ts_col = "timestamp"  # Replace this with your actual timestamp column name if different

if ts_col in df.columns:
    df[ts_col] = pd.to_datetime(df[ts_col], errors="coerce")
    df["hour"] = df[ts_col].dt.hour
    df["dow"] = df[ts_col].dt.dayofweek

label_col = "event_type"
val_frac = 0.2
batch_size = 64
learning_rate = 1e-3
random_seed = 42

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# AI Assistance: Parts of this code were developed and refined with assistance from OpenAI's ChatGPT.
#                ChatGPT helped design class balancing strategy,
#                and dynamic DataLoader generation for tabular classification tasks.
#  SUPERVISED
has_label = (label_col in df.columns)
if has_label:
    df_lbl = df.dropna(subset=[label_col]).copy()
    y_text = df_lbl[label_col].astype(str).values
    classes = sorted(pd.Series(y_text).unique())
    label2id = {c: i for i, c in enumerate(classes)}
    id2label = {i: c for c, i in label2id.items()}
    y = pd.Series(y_text).map(label2id).values

    if len(set(y)) < 2:
        print("[CLF] Only one class found; skipping supervised classification.")
    else:
        cat_keys = ["provider", "region", "honeypot", "src_country", "asn", "ip_reputation", "src_os", "protocol"]
        num_keys = ["dst_port", "bytes_in", "bytes_out", "hour", "dow"]
        cat_cols = [k for k in cat_keys if k in df_lbl.columns]
        num_cols = [k for k in num_keys if k in df_lbl.columns]

        # vocabs
        cat_vocabs = {}
        for c in cat_cols:
            vals = df_lbl[c].astype(str).fillna("na").unique().tolist()
            stoi = {v: i + 1 for i, v in enumerate(sorted(vals))}
            stoi["<unk>"] = 0
            cat_vocabs[c] = stoi

        scaler = StandardScaler(with_mean=False)
        if num_cols:
            scaler.fit(df_lbl[num_cols])

        class ClfDS(Dataset):
            def __init__(self, frame, y):
                self.df = frame.reset_index(drop=True)
                self.y = np.array(y, dtype=np.int64)
            def __len__(self): return len(self.df)
            def __getitem__(self, i):
                r = self.df.iloc[i]
                x_cat = np.array([cat_vocabs[c].get(str(r[c]), 0) for c in cat_cols], dtype=np.int64)
                if num_cols:
                    x_num = scaler.transform([r[num_cols].values.astype(np.float32)])[0].astype(np.float32)
                else:
                    x_num = np.zeros((0,), dtype=np.float32)
                return x_cat, x_num, self.y[i]

        idx_tr, idx_va = train_test_split(np.arange(len(df_lbl)), test_size=val_frac, random_state=random_seed, stratify=y)
        tr_ds = ClfDS(df_lbl.iloc[idx_tr], y[idx_tr])
        va_ds = ClfDS(df_lbl.iloc[idx_va], y[idx_va])

        tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)
        va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False)

        model = TabularNet(cat_cols, cat_vocabs, num_dim=len(num_cols), num_classes=len(classes)).to(device)

        def class_weights(y_ids, K):
            cnt = np.bincount(y_ids, minlength=K).astype(np.float32)
            inv = 1.0 / np.maximum(cnt, 1.0)
            return torch.tensor(inv / inv.mean(), dtype=torch.float32)

        weights = class_weights(y[idx_tr], len(classes)).to(device)
        crit = nn.CrossEntropyLoss(weight=weights)
        opt = torch.optim.Adam(model.parameters(), lr=learning_rate)



# Define the number of epochs manually
epochs = 20  # you can change this to 10, 30, etc.

# Train the classifier
best_f1, best_state, patience, max_pat = -1.0, None, 5, 5

for ep in range(1, epochs + 1):
    model.train()
    seen = 0
    tr_loss = 0.0

    for x_cat, x_num, tgt in tr_loader:
        x_cat, x_num, tgt = x_cat.to(device), x_num.to(device), tgt.to(device)
        opt.zero_grad()
        logits = model(x_cat, x_num)
        loss = crit(logits, tgt)
        loss.backward()
        opt.step()
        tr_loss += loss.item() * tgt.size(0)
        seen += tgt.size(0)

    # Validation
    model.eval()
    y_true, y_hat = [], []
    with torch.no_grad():
        for x_cat, x_num, tgt in va_loader:
            x_cat, x_num = x_cat.to(device), x_num.to(device)
            logits = model(x_cat, x_num)
            y_true.extend(tgt.cpu().numpy())
            y_hat.extend(torch.argmax(logits, 1).cpu().numpy())

    acc = accuracy_score(y_true, y_hat)
    f1 = f1_score(y_true, y_hat, average="macro")
    print(f"[CLF] Epoch {ep:02d} | train_loss={tr_loss / max(1, seen):.4f} | val_acc={acc:.3f} | val_f1={f1:.3f}")

    if f1 > best_f1:
        best_f1 = f1
        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
        patience = max_pat
    else:
        patience -= 1
        if patience <= 0:
            print("[CLF] Early stopping.")
            break

# Load best model weights after training
if best_state is not None:
    model.load_state_dict(best_state)

# Set the output directory
output_dir = "dl_outputs"
os.makedirs(output_dir, exist_ok=True)

# Final report
model.eval()
y_true, y_hat = [], []
with torch.no_grad():
    for x_cat, x_num, tgt in va_loader:
        x_cat, x_num = x_cat.to(device), x_num.to(device)
        logits = model(x_cat, x_num)
        y_true.extend(tgt.cpu().numpy())
        y_hat.extend(torch.argmax(logits, 1).cpu().numpy())

# Classification report
report = classification_report(y_true, y_hat, labels=list(range(len(classes))), target_names=classes, digits=3)
print("\n=== Classification report ===\n", report)

# Save classification report
report_path = os.path.join(output_dir, "classification_report.txt")
with open(report_path, "w") as f:
    f.write(report)

# Confusion matrix
cm = confusion_matrix(y_true, y_hat, labels=list(range(len(classes))))
fig = plt.figure()
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes).plot(xticks_rotation=45)
plt.title("Confusion Matrix (Validation)")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "confusion_matrix.png"))
plt.close(fig)

# === Save artifacts ===
artifact_dir = os.path.join(output_dir, "artifacts")
os.makedirs(artifact_dir, exist_ok=True)

torch.save(model.state_dict(), os.path.join(artifact_dir, "clf_event_type.pt"))

with open(os.path.join(artifact_dir, "clf_meta.json"), "w") as f:
    json.dump({
        "classes": classes,
        "cat_cols": cat_cols,
        "num_cols": num_cols,
        "cat_vocabs": cat_vocabs
    }, f, indent=2)

if num_cols:
    joblib.dump(scaler, os.path.join(artifact_dir, "clf_scaler.joblib"))

print("Saved classifier artifacts to:", artifact_dir)

# AI Assistance: Preprocessing design, particularly the use of sklearn's ColumnTransformer for combining
# one-hot encoded categorical features and scaled numerics, was guided with help from OpenAI's ChatGPT.

#  UNSUPERVISED (Autoencoder)

# One-hot for cats + scaled numerics
cat_keys_ae = ["provider", "region", "honeypot", "src_country", "asn", "ip_reputation", "src_os", "protocol", "cve"]
num_keys_ae = ["dst_port", "bytes_in", "bytes_out", "hour", "dow"]

cat_cols_ae = [k for k in cat_keys_ae if k in df.columns]
num_cols_ae = [k for k in num_keys_ae if k in df.columns]

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

pre_ae = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols_ae),
    ("num", StandardScaler(with_mean=False), num_cols_ae)
])

X = df[cat_cols_ae + num_cols_ae].copy()
X_enc = pre_ae.fit_transform(X)
X_enc = X_enc.toarray().astype(np.float32) if hasattr(X_enc, "toarray") else np.asarray(X_enc, dtype=np.float32)

I# Assistance: This script was developed with the assistance of OpenAI's ChatGPT.
# ChatGPT contributed to model architecture, and anomaly score post-processing using Z-scores and percentiles.
# Define hyperparameters manually
ae_lr = 1e-3
ae_batch = 128
ae_epochs = 20
val_frac = 0.2
random_seed = 42

# Autoencoder model
class AE(nn.Module):
    def __init__(self, in_dim, hidden=[256,128,64]):
        super().__init__()
        enc, last = [], in_dim
        for h in hidden:
            enc += [nn.Linear(last, h), nn.ReLU()]
            last = h
        self.encoder = nn.Sequential(*enc)
        dec, hid = [], hidden[:-1][::-1]
        for h in hid:
            dec += [nn.Linear(last, h), nn.ReLU()]
            last = h
        dec += [nn.Linear(last, in_dim)]
        self.decoder = nn.Sequential(*dec)

    def forward(self, x):
        return self.decoder(self.encoder(x))

# Init model and optimizer
ae = AE(X_enc.shape[1]).to(device)
ae_opt = torch.optim.Adam(ae.parameters(), lr=ae_lr)
ae_crit = nn.MSELoss()

# Split + Loaders
from sklearn.model_selection import train_test_split

idx_tr, idx_va = train_test_split(np.arange(len(X_enc)), test_size=val_frac, random_state=random_seed)
tr_loader = DataLoader(torch.tensor(X_enc[idx_tr]), batch_size=ae_batch, shuffle=True)
va_loader = DataLoader(torch.tensor(X_enc[idx_va]), batch_size=ae_batch, shuffle=False)

# Training loop
best_val, best_state, patience = float("inf"), None, 5
for ep in range(1, ae_epochs + 1):
    ae.train(); tr_loss = 0; seen = 0
    for xb in tr_loader:
        xb = xb.to(device)
        ae_opt.zero_grad()
        recon = ae(xb)
        loss = ae_crit(recon, xb)
        loss.backward()
        ae_opt.step()
        tr_loss += loss.item() * xb.size(0)
        seen += xb.size(0)
    tr_m = tr_loss / max(1, seen)

    # Validation
    ae.eval(); va_loss = 0; vseen = 0
    with torch.no_grad():
        for xb in va_loader:
            xb = xb.to(device)
            recon = ae(xb)
            loss = ae_crit(recon, xb)
            va_loss += loss.item() * xb.size(0)
            vseen += xb.size(0)
    va_m = va_loss / max(1, vseen)
    print(f"[AE] Epoch {ep:02d} | train={tr_m:.6f} | val={va_m:.6f}")

    if va_m < best_val:
        best_val = va_m
        best_state = {k: v.detach().cpu().clone() for k, v in ae.state_dict().items()}
        patience = 5
    else:
        patience -= 1
        if patience <= 0:
            print("[AE] Early stopping.")
            break

#  Load best model
if best_state is not None:
    ae.load_state_dict(best_state)

# Inference + Anomaly Scoring
with torch.no_grad():
    recon = ae(torch.tensor(X_enc, device=device)).cpu().numpy()
recon_err = ((X_enc - recon) ** 2).mean(axis=1)

df["anomaly_score"] = recon_err
df["anomaly_percentile"] = pd.Series(recon_err).rank(pct=True).values
mu, sd = float(np.mean(recon_err)), float(np.std(recon_err) + 1e-8)
df["anomaly_z"] = (recon_err - mu) / sd

df.sort_values("anomaly_score", ascending=False).head(10)

# Check if 'anomaly_score' column exists
if "anomaly_score" in df.columns:
    plt.figure(figsize=(10, 6))
    plt.hist(df["anomaly_score"], bins=50, color="skyblue", edgecolor="black")
    plt.title("Distribution of Anomaly Scores")
    plt.xlabel("Anomaly Score")
    plt.ylabel("Number of Events")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
else:
    print("Column 'anomaly_score' not found in the dataframe.")

plt.figure(figsize=(12, 5))
plt.scatter(df["timestamp"], df["anomaly_score"], alpha=0.5, c='red')
plt.title("Anomaly Score Over Time")
plt.xlabel("Timestamp")
plt.ylabel("Anomaly Score")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Set output directory manually
outdir = "dl_outputs"
os.makedirs(outdir, exist_ok=True)

# Save full scored dataset
scored_csv = os.path.join(outdir, "anomalies_autoencoder_scored.csv")
df.to_csv(scored_csv, index=False)

# Select important columns to keep
keep_cols = [
    "timestamp", "provider", "region", "honeypot", "src_ip", "src_country", "asn", "ip_reputation", "cve", "dst_port",
    "anomaly_score", "anomaly_percentile", "anomaly_z"
]
keep_cols = [c for c in keep_cols if c in df.columns]  # filter out any missing ones

# Save Top 20 anomalies
top20 = df.sort_values("anomaly_score", ascending=False).head(20)[keep_cols]
top_csv = os.path.join(outdir, "anomalies_top20.csv")
top20.to_csv(top_csv, index=False)

print(f"Full dataset saved to: {scored_csv}")
print(f" Top 20 anomalies saved to: {top_csv}")

# Set output directory
outdir = "dl_outputs"
os.makedirs(outdir, exist_ok=True)

# Save AE artifacts
art = os.path.join(outdir, "artifacts")
os.makedirs(art, exist_ok=True)

torch.save(ae.state_dict(), os.path.join(art, "ae.pt"))
joblib.dump(pre_ae, os.path.join(art, "ae_preprocessor.joblib"))

# Print saved files
print("\n=== Saved Outputs ===")
if has_label and os.path.exists(os.path.join(outdir, "classification_report.txt")):
    print("- classification_report.txt")
    print("- confusion_matrix.png")
    print("- artifacts/clf_event_type.pt + clf_meta.json (+ clf_scaler.joblib)")
print("- anomalies_autoencoder_scored.csv")
print("- anomalies_top20.csv")
print("- artifacts/ae.pt + ae_preprocessor.joblib")
print(f"\nAll files saved in: {os.path.abspath(outdir)}")

# Define simple MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

model = MLP(input_size=X.shape[1], hidden_size=16, num_classes=len(np.unique(y)))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Train for a few epochs
for epoch in range(10):
    optimizer.zero_grad()
    output = model(X_train_tensor)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()

# SHAP explanation using DeepExplainer
explainer = shap.DeepExplainer(model, X_train_tensor[:50])
shap_values = explainer.shap_values(X_test_tensor[:10])  # smaller subset for speed

# SHAP summary plot
shap.summary_plot(shap_values, X_test_tensor[:10].numpy(), feature_names=data.columns[:-1])

!pip install shap

#AI Assistance: ChatGPT was used to design the SHAP integration steps, including explainer setup,
# SHAP value computation.

# Step 2: Import SHAP and relevant libraries
import shap
import torch
import numpy as np
import matplotlib.pyplot as plt
feature_names = [
    "cloud_provider",          # AWS, Azure, GCP (encoded)
    "honeypot_type",           # Cowrie, Dionaea, etc.
    "region",                  # us-east1, europe-west2, etc.
    "source_country",          # e.g., CN, US, RU
    "asn",                     # Autonomous System Number
    "reputation_score",        # IP reputation score or category
    "protocol",                # TCP, UDP, etc. (encoded)
    "dst_port",                # Target port (numeric)
    "bytes_in",                # Number of incoming bytes
    "bytes_out",               # Number of outgoing bytes
    "hour",                    # Time of day (0–23)
    "day_of_week",             # 0–6 for Monday–Sunday
    "cve_severity",            # Mapped from Suricata CVE signature
    "event_count_by_ip",       # Frequency of this IP in logs
    "avg_session_duration"     # If available, else remove
]


# Step 3: Use a subset of training data for background distribution
background = X_train_tensor[:50]

# Step 4: Choose a small batch of test data to explain
data_to_explain = X_test_tensor[:10]

# Step 5: Create the SHAP explainer
explainer = shap.DeepExplainer(model, background)

# Step 6: Compute SHAP values
shap_values = explainer.shap_values(data_to_explain)

# Step 7: Convert tensor to NumPy for plotting
X_np = data_to_explain.detach().cpu().numpy()

# Step 8: Plot SHAP summary (beeswarm)
shap.summary_plot(shap_values, X_np, feature_names=feature_names)

plt.gcf().set_size_inches(10, 5)  # Optional: adjust figure size
shap.summary_plot(shap_values, X_np, feature_names=feature_names, show=False)
plt.savefig("shap_beeswarm_honeypot.png", dpi=300, bbox_inches='tight')

shap.summary_plot(shap_values, X_np, feature_names=feature_names, plot_type="bar")
plt.savefig("shap_dependence_bytes_in.png", dpi=300, bbox_inches='tight')



